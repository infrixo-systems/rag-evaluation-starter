{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# rag-evaluation-starter \u2014 Walkthrough\n",
    "\n",
    "**Evaluate any RAG pipeline against a golden set in under 30 minutes.**\n",
    "\n",
    "This notebook walks you through every step: loading a golden set, plugging in your retriever and generator, running the evaluation, and interpreting the results.\n",
    "\n",
    "---\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| \ud83d\udd17 GitHub | [github.com/infrixo-systems/rag-evaluation-starter](https://github.com/infrixo-systems/rag-evaluation-starter) |\n",
    "| \u23f1 Time to complete | ~20 minutes |\n",
    "| \ud83d\udd11 API key required? | No \u2014 works with the mock retriever out of the box |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install the dependencies and clone the repo. `sentence-transformers` is needed for the cosine-similarity relevance metric. If you skip it, pass `no_embeddings=True` to `evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# \u2500\u2500 Colab: clone the repo and move into it \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"sentence-transformers\", \"tiktoken\", \"rich\", \"numpy\", \"-q\"])\n",
    "    if not os.path.exists(\"rag-evaluation-starter\"):\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/infrixo-systems/rag-evaluation-starter.git\"])\n",
    "    os.chdir(\"rag-evaluation-starter\")\n",
    "\n",
    "# \u2500\u2500 Local: find repo root (works whether you launch from repo root or notebook/)\n",
    "else:\n",
    "    # Walk up directory tree until we find rag_eval.py\n",
    "    here = os.path.abspath(\".\")\n",
    "    root = here\n",
    "    for _ in range(5):  # max 5 levels up\n",
    "        if os.path.exists(os.path.join(root, \"rag_eval.py\")):\n",
    "            break\n",
    "        root = os.path.dirname(root)\n",
    "    os.chdir(root)\n",
    "    if root not in sys.path:\n",
    "        sys.path.insert(0, root)\n",
    "\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "assert os.path.exists(\"rag_eval.py\"), \"Could not find rag_eval.py \u2014 please launch Jupyter from the repo root\"\n",
    "print(\"\u2705 Setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the evaluation library\n",
    "from rag_eval import (\n",
    "    load_golden_set,\n",
    "    evaluate,\n",
    "    summarise,\n",
    "    print_rich_table,\n",
    "    save_json,\n",
    ")\n",
    "\n",
    "print('\u2705 Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-set-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and Inspect the Golden Set\n",
    "\n",
    "A **golden set** is a hand-curated list of question \u2192 expected_answer pairs, each annotated with the source document(s) the answer should come from.\n",
    "\n",
    "Think of it as a regression test suite for your RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-set-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden = load_golden_set('examples/golden_set.json')\n",
    "\n",
    "print(f'Loaded {len(golden)} questions\\n')\n",
    "print(json.dumps(golden[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-set-schema",
   "metadata": {},
   "source": [
    "### Golden set field reference\n",
    "\n",
    "| Field | Required | What it's for |\n",
    "|---|---|---|\n",
    "| `id` | \u2705 | Stable identifier \u2014 used to track regressions over time |\n",
    "| `question` | \u2705 | The question exactly as a user would ask it |\n",
    "| `expected_answer` | \u2705 | Reference answer for Exact Match and Token F1 scoring |\n",
    "| `expected_source_ids` | \u2705 | Which document chunk(s) should be retrieved |\n",
    "| `category` | Recommended | Groups results \u2014 e.g. `billing`, `api`, `onboarding` |\n",
    "| `difficulty` | Recommended | `easy` / `medium` / `hard` \u2014 lets you see where the system breaks first |\n",
    "| `notes` | Optional | Internal context about why this question is in the set |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-set-explore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary of the example golden set\n",
    "from collections import Counter\n",
    "\n",
    "difficulties = Counter(e.get('difficulty') for e in golden)\n",
    "categories   = Counter(e.get('category')   for e in golden)\n",
    "\n",
    "print('By difficulty:', dict(difficulties))\n",
    "print('By category:  ', dict(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retriever-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Your Retriever\n",
    "\n",
    "The retriever must be a function with this signature:\n",
    "```python\n",
    "def my_retriever(question: str) -> list[dict]:\n",
    "    # Returns a list of {\"text\": ..., \"source_id\": ...} dicts\n",
    "    ...\n",
    "```\n",
    "\n",
    "Below are three real-world examples. **Copy the one that matches your stack.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retriever-langchain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Option A: LangChain + any vector store \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "#\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "#\n",
    "# vectorstore = Chroma(\n",
    "#     persist_directory=\"./my_chroma_db\",\n",
    "#     embedding_function=OpenAIEmbeddings(),\n",
    "# )\n",
    "#\n",
    "# def my_retriever(question: str) -> list[dict]:\n",
    "#     docs = vectorstore.similarity_search(question, k=3)\n",
    "#     return [\n",
    "#         {\"text\": d.page_content, \"source_id\": d.metadata.get(\"source\", \"unknown\")}\n",
    "#         for d in docs\n",
    "#     ]\n",
    "\n",
    "print('LangChain retriever (commented out \u2014 uncomment and fill in your vectorstore)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retriever-llamaindex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Option B: LlamaIndex \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "#\n",
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "#\n",
    "# documents = SimpleDirectoryReader(\"./my_docs\").load_data()\n",
    "# index     = VectorStoreIndex.from_documents(documents)\n",
    "# retriever = index.as_retriever(similarity_top_k=3)\n",
    "#\n",
    "# def my_retriever(question: str) -> list[dict]:\n",
    "#     nodes = retriever.retrieve(question)\n",
    "#     return [\n",
    "#         {\"text\": n.text, \"source_id\": n.metadata.get(\"file_name\", \"unknown\")}\n",
    "#         for n in nodes\n",
    "#     ]\n",
    "\n",
    "print('LlamaIndex retriever (commented out \u2014 uncomment and fill in your index path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retriever-chroma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Option C: Raw ChromaDB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "#\n",
    "# import chromadb\n",
    "# from chromadb.utils import embedding_functions\n",
    "#\n",
    "# client     = chromadb.PersistentClient(path=\"./my_chroma_db\")\n",
    "# collection = client.get_collection(\n",
    "#     \"my_docs\",\n",
    "#     embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction()\n",
    "# )\n",
    "#\n",
    "# def my_retriever(question: str) -> list[dict]:\n",
    "#     results = collection.query(query_texts=[question], n_results=3)\n",
    "#     texts   = results[\"documents\"][0]\n",
    "#     metas   = results[\"metadatas\"][0]\n",
    "#     return [\n",
    "#         {\"text\": t, \"source_id\": m.get(\"source\", \"unknown\")}\n",
    "#         for t, m in zip(texts, metas)\n",
    "#     ]\n",
    "\n",
    "print('Raw ChromaDB retriever (commented out \u2014 uncomment and fill in your collection)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retriever-mock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Default: mock retriever (works without any live system) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "from examples.mock_retriever import mock_retriever_fn as my_retriever\n",
    "\n",
    "# Quick test\n",
    "sample = my_retriever(\"What is the rate limit for the Vanta Billing API?\")\n",
    "print(f'Retrieved {len(sample)} chunks')\n",
    "print('Top result source_id:', sample[0]['source_id'])\n",
    "print('Top result text excerpt:', sample[0]['text'][:80], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Define Your Generator\n",
    "\n",
    "The generator must be a function with this signature:\n",
    "```python\n",
    "def my_generator(question: str, context: list[str]) -> str:\n",
    "    # context is a list of retrieved text chunks\n",
    "    # Returns the generated answer as a string\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator-openai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Option A: OpenAI \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "#\n",
    "# import openai, os\n",
    "# client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "#\n",
    "# def my_generator(question: str, context: list[str]) -> str:\n",
    "#     ctx_text = \"\\n\\n\".join(context)\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": f\"Answer using only the context below.\\n\\n{ctx_text}\"},\n",
    "#             {\"role\": \"user\",   \"content\": question},\n",
    "#         ],\n",
    "#         temperature=0,\n",
    "#         max_tokens=300,\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "print('OpenAI generator (commented out)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator-anthropic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Option B: Anthropic \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "#\n",
    "# import anthropic, os\n",
    "# client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "#\n",
    "# def my_generator(question: str, context: list[str]) -> str:\n",
    "#     ctx_text = \"\\n\\n\".join(context)\n",
    "#     response = client.messages.create(\n",
    "#         model=\"claude-haiku-4-5-20251001\",\n",
    "#         max_tokens=300,\n",
    "#         system=f\"Answer using only the context below.\\n\\n{ctx_text}\",\n",
    "#         messages=[{\"role\": \"user\", \"content\": question}],\n",
    "#     )\n",
    "#     return response.content[0].text\n",
    "\n",
    "print('Anthropic generator (commented out)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator-hf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Option C: HuggingFace (fully local) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "#\n",
    "# from transformers import pipeline\n",
    "# pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "#\n",
    "# def my_generator(question: str, context: list[str]) -> str:\n",
    "#     ctx_text = \"\\n\\n\".join(context)\n",
    "#     prompt = f\"Context:\\n{ctx_text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "#     output = pipe(prompt, max_new_tokens=200, temperature=0.1)\n",
    "#     return output[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "\n",
    "print('HuggingFace generator (commented out)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator-mock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Default: mock generator \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "from examples.mock_retriever import mock_generator_fn as my_generator\n",
    "\n",
    "# Quick test\n",
    "context = [c['text'] for c in my_retriever(\"What is the rate limit?\")]\n",
    "answer  = my_generator(\"What is the rate limit for the Vanta Billing API free tier?\", context)\n",
    "print('Answer:', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-eval-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run the Evaluation\n",
    "\n",
    "One function call. `evaluate()` loops through the golden set, calls your retriever and generator, computes all five metrics, and returns a structured list of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(\n",
    "    golden_set   = golden,\n",
    "    retriever_fn = my_retriever,\n",
    "    generator_fn = my_generator,\n",
    "    k            = 3,           # top-K for retrieval recall\n",
    "    no_embeddings= True,        # set False if sentence-transformers is installed\n",
    ")\n",
    "\n",
    "summary = summarise(results)\n",
    "print(f'Evaluated {summary[\"n_questions\"]} questions')\n",
    "print('Mean scores:', json.dumps(summary['mean_scores'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Inspect Results\n",
    "\n",
    "### 6a. Rich console table\n",
    "\n",
    "The table shows per-question scores colour-coded as **PASS** (green), **WARN** (amber), **FAIL** (red), with a summary row at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rich_table(results, summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-filter",
   "metadata": {},
   "source": [
    "### 6b. Filter by category and difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-filter-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out just the result rows (excludes the _summary sentinel)\n",
    "rows = [r for r in results if not r.get('_summary')]\n",
    "\n",
    "# Filter to hard questions only\n",
    "hard = [r for r in rows if r.get('difficulty') == 'hard']\n",
    "print(f'Hard questions: {len(hard)}')\n",
    "for r in hard:\n",
    "    f1 = r['metrics']['token_f1']['score']\n",
    "    recall = r['metrics']['retrieval_recall_at_k']['score']\n",
    "    print(f\"  {r['id']} | Recall@3={recall:.2f} | Token F1={f1:.2f} | Q: {r['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-by-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Token F1 by category\n",
    "from collections import defaultdict\n",
    "\n",
    "cat_scores = defaultdict(list)\n",
    "for r in rows:\n",
    "    cat_scores[r.get('category', 'unknown')].append(r['metrics']['token_f1']['score'])\n",
    "\n",
    "print('Mean Token F1 by category:')\n",
    "for cat, scores in sorted(cat_scores.items()):\n",
    "    print(f'  {cat:<15} {sum(scores)/len(scores):.3f}  (n={len(scores)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpret-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Interpret Your Scores\n",
    "\n",
    "Raw numbers without context are hard to act on. Here's what each metric actually means:\n",
    "\n",
    "---\n",
    "\n",
    "### Retrieval Recall@K\n",
    "**What it measures:** Did the right chunk come back in the top-K results?\n",
    "\n",
    "| Score | What it means | What to do |\n",
    "|---|---|---|\n",
    "| 0.8 \u2013 1.0 | Retrieval is working well | Focus on generation quality |\n",
    "| 0.5 \u2013 0.8 | Some queries miss the right chunk | Check chunk size, embedding model |\n",
    "| < 0.5 | Retrieval is broken for many queries | Fix before touching generation |\n",
    "\n",
    "**A Recall@3 of 0.6 means:** 40% of questions didn't find the right source in the top 3 results. Your generator is being asked to answer from wrong context \u2014 no amount of prompt-tuning will fix that.\n",
    "\n",
    "---\n",
    "\n",
    "### Answer Faithfulness\n",
    "**What it measures:** Is the answer grounded in the retrieved context?\n",
    "\n",
    "Low faithfulness + high retrieval recall \u2192 your generator is **hallucinating** even when the right context is present.\n",
    "Low faithfulness + low retrieval recall \u2192 the generator is making things up because it received bad context.\n",
    "\n",
    "---\n",
    "\n",
    "### Answer Relevance (cosine similarity)\n",
    "**What it measures:** Is the answer semantically on-topic for the question?\n",
    "\n",
    "A system can retrieve the right chunks and generate a grounded answer that still doesn't actually address the question. This metric catches that.\n",
    "\n",
    "---\n",
    "\n",
    "### Token F1 / Exact Match\n",
    "**What it measures:** How closely does the generated answer match the reference answer?\n",
    "\n",
    "Most useful for **factual Q&A** where the answer is a specific value (a number, a date, a policy rule). Less useful for open-ended questions where multiple phrasings are valid.\n",
    "\n",
    "---\n",
    "\n",
    "### Latency + Cost\n",
    "**What it measures:** Time per query. Token cost estimate (if `--llm-judge` is used).\n",
    "\n",
    "Use this to catch regressions before deploying \u2014 a new embedding model might improve recall but triple latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpret-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep-dive into a single result\n",
    "r = rows[4]  # q005 \u2014 the hard backdating question\n",
    "\n",
    "print(f\"Question:  {r['question']}\")\n",
    "print(f\"Expected:  {r['expected_answer'][:80]}...\")\n",
    "print(f\"Generated: {r['generated_answer'][:80]}...\")\n",
    "print()\n",
    "print('Retrieved sources:')\n",
    "for chunk in r['retrieved_chunks']:\n",
    "    print(f\"  [{chunk['source_id']}] {chunk['text'][:60]}...\")\n",
    "print()\n",
    "for metric, data in r['metrics'].items():\n",
    "    score = data['score']\n",
    "    v     = data['verdict']\n",
    "    print(f\"  {metric:<30} {score!s:>6}  [{v}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(results, summary, 'my_results.json')\n",
    "print('Saved \u2192 my_results.json')\n",
    "\n",
    "# Also save as CSV\n",
    "from rag_eval import save_csv\n",
    "save_csv(results, 'my_results.csv')\n",
    "print('Saved \u2192 my_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Next Steps\n",
    "\n",
    "### If Retrieval Recall is low\n",
    "- Try smaller chunk sizes (512 \u2192 256 tokens)\n",
    "- Try a better embedding model (`BAAI/bge-large-en-v1.5` often beats MiniLM)\n",
    "- Add a reranker (CrossEncoder) on top of your retriever\n",
    "\n",
    "### If Faithfulness is low\n",
    "- Strengthen your system prompt: *\"Answer ONLY using the context below. If the answer is not in the context, say you don't know.\"*\n",
    "- Reduce max_tokens to discourage the model from padding with hallucinations\n",
    "\n",
    "### If Token F1 is low on hard questions\n",
    "- Review those questions in your golden set \u2014 some may need updated expected answers\n",
    "- Consider adding a reranker to surface the most relevant chunks first\n",
    "\n",
    "### When you've outgrown this script\n",
    "This tool is designed for diagnostic evaluation against a fixed golden set. When you need continuous eval in production, look at:\n",
    "- [Ragas](https://docs.ragas.io) \u2014 comprehensive RAG metrics framework\n",
    "- [LangSmith](https://smith.langchain.com) \u2014 evaluation + tracing for LangChain apps\n",
    "- [TruLens](https://www.trulens.org) \u2014 feedback functions for LLM apps\n",
    "\n",
    "---\n",
    "\n",
    "*If your scores are consistently low and you're not sure why, this is the kind of thing we look at in a Foundation Check \u2014 [infrixo.com/start](https://infrixo.com/start)*"
   ]
  }
 ]
}